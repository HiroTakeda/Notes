{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOX2C1EZSXC4AMfkSkdk9fC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiroML/test/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5NLxpIU7GWe"
      },
      "source": [
        "# Logistic regression\n",
        "\n",
        "The 1-D logistic regression problem may be stated as:\n",
        "\n",
        "> Given a set of data $(x_n, t_n)$ for $n = 1$ to $N$, where $t_n=f(x_n)$ is either __true__ or __false__, find the function $f$.\n",
        "\n",
        "It can be regarded as a binary classification problem, where we can define the classifier function $f(x)$ that returns __true__ if the posterior probability $p(\\mathrm{true}\\, |\\, x) > 0.5$:\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "f(x) &=& \\mathrm{true} \\ \\ \\mathrm{if} \\ \\  p(\\mathrm{true}\\, |\\, x) \\,>\\, \\gamma \\nonumber \\\\\n",
        "f(x) &=& \\mathrm{false} \\ \\ \\mathrm{if} \\ \\ p(\\mathrm{false}\\, |\\, x) \\,\\ge\\, \\gamma, \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is a threshold value. Now, the poterior distribution $p$ is of our interest. Because this is a binary classification problem, by letting $p(\\mathrm{true}\\,|\\,x)=g(x)$ we have\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "p(\\mathrm{true}\\,|\\,x) &=& g(x) \\nonumber \\\\\n",
        "p(\\mathrm{false}\\,|\\,x) &=& 1 - g(x). \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "The maximum likelihood method is a suitable choice to find the function $g(x)$. Letting $t_n=1$ for __true__ and $t_n=0$ for __false__ samples, $p$ becomes the Bernoulli distribution and we have the likelihood function:\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "L &=& \\displaystyle\\prod_{n=1}^{N}p(\\mathrm{true}\\,|\\,x_n)^{t_n} \\, p(\\mathrm{false}\\,|\\,x_n)^{1-t_n} \\nonumber \\\\\n",
        "&=& \\displaystyle\\prod_{n=1}^N g(x_n)^{t_n} (1-g(x_n))^{1-t_n}. \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "Taking the logarighm of both sides, we have\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\ln L &=& \\ln \\displaystyle\\prod_{n=1}^N g(x_n)^{t_n} (1-g(x_n))^{1-t_n} \\nonumber \\\\\n",
        "&=& \\displaystyle\\sum_{n=0}^N t_n \\ln g(x_n) + \\displaystyle\\sum_{n=0}^N (1-t_n) \\ln (1-g(x_n)) \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "Similar to the regression problem, let's model the function $g$ with polinomials:\n",
        "\n",
        "$$\n",
        "g(x) = a(w_0 + w_1 x + w_2 x^2 + \\cdots) = a(\\mathbf{x}^T \\mathbf{w}).\n",
        "$$\n",
        "\n",
        "where $\\mathbf{x}=[1,x,x^2,\\cdots]^T$, $\\mathbf{w}=[w_0,w_1,w_2,\\cdots]^T$, and a function $a(\\cdot)$ confines the output value within $0$ and $1$, such as the __sigmoid__ function. In general, $g(x)$ would be modeled by __neural network__.\n",
        "\n",
        "The unknown parameter $\\mathbf{w}$ can be estimated by maximizing the log likelihood function:\n",
        "\n",
        "$$\n",
        "\\ln L(\\mathbf{w}) = \\displaystyle\\sum_{n=0}^N t_n \\ln a(\\mathbf{x}_n^T \\mathbf{w}) + \\displaystyle\\sum_{n=0}^N (1-t_n) \\ln (1-a(\\mathbf{x}_n^T \\mathbf{w}))\n",
        "$$\n",
        "\n",
        "The gradient descent method would be a simple choice to find the parameter $\\mathbf{w}$ because of the logistic function $a(\\cdot)$. In order to use the gradient descent method, we take the gradient of the log likelihood:\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\color{orange}{\\frac{\\partial}{\\partial\\mathbf{w}} \\ln L(\\mathbf{w})} &=& \\displaystyle\\sum_{n=0}^N t_n \\color{#0033CC}{\\frac{\\partial}{\\partial\\mathbf{w}} \\ln a(\\mathbf{x}_n^T \\mathbf{w})} + \\displaystyle\\sum_{n=0}^N (1-t_n) \\color{#02B02B}{\\frac{\\partial}{\\partial\\mathbf{w}} \\ln (1-a(\\mathbf{x}_n^T \\mathbf{w}))} \\nonumber \\\\\n",
        "&=& \\displaystyle\\sum_{n=1}^N (t_n - a(\\mathbf{x}_n^T \\mathbf{w}))\\, \\mathbf{x}_n, \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "where $a(\\cdot)=\\mathrm{sigmoid}(\\cdot)$,\n",
        "\n",
        "$$\n",
        "\\color{#0033CC}{\n",
        "\\displaystyle\\frac{\\partial}{\\partial \\mathbf{w}} \\ln a(\\mathbf{x}_n^T \\mathbf{w}) = \\frac{\\frac{\\partial}{\\partial \\mathbf{w}} a(\\mathbf{x}_n^T \\mathbf{w})}{a(\\mathbf{x}_n^T \\mathbf{w})} = \\frac{a(\\mathbf{x}_n^T \\mathbf{w}) (1 - a(\\mathbf{x}_n^T \\mathbf{w})) \\mathbf{x}_n}{a(\\mathbf{x}_n^T \\mathbf{w})} = (1 - a(\\mathbf{x}_n^T \\mathbf{w})) \\mathbf{x}_n},\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\color{#02B02B}{\n",
        "\\frac{\\partial}{\\partial\\mathbf{w}} \\ln (1-a(\\mathbf{x}_n^T \\mathbf{w})) = \\frac{\\frac{\\partial}{\\partial \\mathbf{w}} (1 - a(\\mathbf{x}_n^T \\mathbf{w}))}{1 -a(\\mathbf{x}_n^T \\mathbf{w})} = -a(\\mathbf{x}_n^T \\mathbf{w}) \\mathbf{x}_n}.\n",
        "$$\n",
        "\n",
        "The gradient descent method yields an solution by iteratively update the parameter:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(\\ell+1)} = \\mathbf{w}^{(\\ell)} + \\lambda \\color{orange}{\\frac{\\partial}{\\partial \\mathbf{w}} \\ln L(\\mathbf{w})}.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FchUGeZA7FGc"
      },
      "source": [
        "import numpy\n",
        "import pylab\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}