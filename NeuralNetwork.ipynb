{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNogQ34Tl1odXXs1s5hsvO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiroTakeda/Notes/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFIHt7neGU_I"
      },
      "source": [
        "# Nerual Network\n",
        "\n",
        "+ Related topics: __regression__, __logistic regression__\n",
        "\n",
        "Neural network is a universal function approximator. This note shows how to implement the __linear layer__ (also known as __dense layer__ or __fully connected layer__), one of the basic component of the neural network approach, and it is applicalble to both __regression__ and __logistic regression__ problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsSqC4mMvMMy"
      },
      "source": [
        "## 2-Layer Neural Network\n",
        "![picture](https://drive.google.com/uc?id=1_p7BO63LIfo9X8K52RkCuZw68A6_MIZt)\n",
        "### Feed-forward network functions\n",
        "\n",
        "The 1st layer\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "a_1 &=& w_{11}^{(1)} x_1 + w_{12}^{(1)} x_2 + \\beta_1^{(1)}, \\quad z_1 &=& h(a_1) \\nonumber \\\\\n",
        "a_2 &=& w_{21}^{(1)} x_1 + w_{22}^{(1)} x_2 + \\beta_2^{(1)}, \\quad z_2 &=& h(a_2) \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "The 2nd layer\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "b_1 &=& w_{11}^{(2)} z_1 + w_{12}^{(2)} z_2 + \\beta_1^{(2)}, \\quad y_1 &=& h(b_1) \\nonumber \\\\\n",
        "b_2 &=& w_{21}^{(2)} z_1 + w_{22}^{(2)} z_2 + \\beta_2^{(2)}, \\quad y_2 &=& h(b_2) \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KW3W2dyGuNw"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "Given a set of training data $(\\mathbf{x}, \\mathbf{t})_n = ([x_1, x_2]^T, [t_1, t_2]^T)_n$ for $n=1,\\cdots N$, we'd like to find the weights and bias values for each layer. Often, the number of samples ($N$) is very large, and the stochastic gradient descent method is more efficient than the gradient descent. Unlike the gradient descent method, the stochastic gradient descent method evaluates one sample at a time. Let's us one of the basic loss function, mean square error, here. For one sample, we have\n",
        "\n",
        "$$\n",
        "E = \\displaystyle\\frac{1}{2} (y_1 - t_1)^2 + \\displaystyle\\frac{1}{2} (y_2 - t_2)^2 = \\displaystyle\\frac{1}{2} \\displaystyle\\sum_{k} (y_k - t_k)^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l7OQ4KUNQc1"
      },
      "source": [
        "### Stochastic gradient descent\n",
        "\n",
        "The stochastic gradient descent method finds the weights and bias values by update iteratively using one sample at a time (a small set of samples are usually used):\n",
        "\n",
        "$$\n",
        "w_{ij}^{(\\cdot)} \\Leftarrow w_{ij}^{(\\cdot)} + \\eta \\displaystyle\\frac{\\partial E}{\\partial w_{ij}^{(\\cdot)}}, \\quad \\beta_{i}^{(\\cdot)} \\Leftarrow \\beta_{i}^{(\\cdot)} + \\eta \\displaystyle\\frac{\\partial E}{\\partial \\beta_{i}^{(\\cdot)}}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is a step size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA97EG8ePpld"
      },
      "source": [
        "### Error backpropagation\n",
        "\n",
        "The linear layers are cascaded, and the outputs of one layer are passed to the follwoing layer. To compute the gradients of weights and bias values, we can use the error backpropagation technique, in which the gradient of each weight can be computed by the chain rule.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The 2nd layer\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{11}^{(2)}} &=& \\displaystyle\\frac{\\partial E}{\\partial y_1} \\displaystyle\\frac{\\partial y_1}{\\partial w_{11}^{(2)}} = \\displaystyle\\frac{\\partial E}{\\partial y_1} \\displaystyle\\frac{\\partial y_1}{\\partial b_1} \\displaystyle\\frac{\\partial b_1}{\\partial w_{11}^{(2)}} = (y_1 - t_1) \\, g(b_1) \\, z_1 = \\delta_1 \\, g(b_1) \\, z_1, \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{12}^{(2)}} &=& \\displaystyle\\frac{\\partial E}{\\partial y_1} \\displaystyle\\frac{\\partial y_1}{\\partial w_{12}^{(2)}} = \\displaystyle\\frac{\\partial E}{\\partial y_1} \\displaystyle\\frac{\\partial y_1}{\\partial b_1} \\displaystyle\\frac{\\partial b_1}{\\partial w_{12}^{(2)}} = (y_1 - t_1) \\, g(b_1) \\, z_2 = \\delta_1 \\, g(b_1) \\, z_2, \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial \\beta_{1}^{(2)}} &=& \\displaystyle\\frac{\\partial E}{\\partial y_1} \\displaystyle\\frac{\\partial y_1}{\\partial \\beta_{1}^{(2)}} = \\displaystyle\\frac{\\partial E}{\\partial y_1} \\displaystyle\\frac{\\partial y_1}{\\partial b_1} \\displaystyle\\frac{\\partial b_1}{\\partial \\beta_{1}^{(2)}} = (y_1 - t_1) \\, g(b_1) = \\delta_1 \\, g(b_1), \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{21}^{(2)}} &=& \\displaystyle\\frac{\\partial E}{\\partial y_2} \\displaystyle\\frac{\\partial y_2}{\\partial w_{21}^{(2)}} = \\displaystyle\\frac{\\partial E}{\\partial y_2} \\displaystyle\\frac{\\partial y_2}{\\partial b_2} \\displaystyle\\frac{\\partial b_2}{\\partial w_{21}^{(2)}} = (y_2 - t_2) \\, g(b_1) \\, z_1 = \\delta_2 \\, g(b_1) \\, z_1, \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{22}^{(2)}} &=& \\displaystyle\\frac{\\partial E}{\\partial y_2} \\displaystyle\\frac{\\partial y_2}{\\partial w_{22}^{(2)}} = \\displaystyle\\frac{\\partial E}{\\partial y_2} \\displaystyle\\frac{\\partial y_2}{\\partial b_2} \\displaystyle\\frac{\\partial b_2}{\\partial w_{22}^{(2)}} = (y_2 - t_2) \\, g(b_1) \\, z_2 = \\delta_2 \\, g(b_1) \\, z_2, \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial \\beta_{2}^{(2)}} &=& \\displaystyle\\frac{\\partial E}{\\partial y_2} \\displaystyle\\frac{\\partial y_2}{\\partial \\beta_{2}^{(2)}} = \\displaystyle\\frac{\\partial E}{\\partial y_2} \\displaystyle\\frac{\\partial y_2}{\\partial b_2} \\displaystyle\\frac{\\partial b_2}{\\partial \\beta_{2}^{(2)}} = (y_2 - t_2) \\, g(b_2) = \\delta_2 \\, g(b_2), \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "where $g(a) = \\frac{\\partial}{\\partial a} h(a)$, for example when $h$ is the sigmoid function, $g(a) = h(a) (1 - h(a))$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3GeXd624ZpW"
      },
      "source": [
        "Therefore, we have\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{11}^{(2)}} & \\displaystyle\\frac{\\partial E}{\\partial w_{12}^{(2)}} \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{21}^{(2)}} & \\displaystyle\\frac{\\partial E}{\\partial w_{22}^{(2)}}\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "\\delta_1 g(b_1) z_1 & \\delta_1 g(b_1) z_2 \\\\\n",
        "\\delta_2 g(b_2) z_1 & \\delta_2 g(b_2) z_2\n",
        "\\end{bmatrix} = \\left(\n",
        "\\begin{bmatrix}\n",
        "g(b_1) \\\\\n",
        "g(b_2)\n",
        "\\end{bmatrix} \\odot\n",
        "\\begin{bmatrix}\n",
        "\\delta_1 \\\\\n",
        "\\delta_2\n",
        "\\end{bmatrix} \\right)\n",
        "\\begin{bmatrix}\n",
        "z_1 & z_2\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\displaystyle\\frac{\\partial E}{\\partial \\beta_{1}^{(2)}} \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial \\beta_{2}^{(2)}}\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "\\delta_1 g(b_1) \\\\\n",
        "\\delta_2 g(b_2)\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "g(b_1) \\\\\n",
        "g(b_2)\n",
        "\\end{bmatrix} \\odot\n",
        "\\begin{bmatrix}\n",
        "\\delta_1 \\\\\n",
        "\\delta_2\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLkv7kRgVBD6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The 1st layer\n",
        "\n",
        "$$\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{11}^{(1)}} = \\displaystyle\\frac{\\partial E}{\\partial a_1} \\displaystyle\\frac{\\partial a_1}{\\partial w_{11}^{(1)}} = \\left\\{ \\delta_1\\, g(b_1) \\, w_{11}^{(2)} + \\delta_2\\, g(b_2) \\, w_{21}^{(2)} \\right\\} g(a_1)\\, x_1\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\displaystyle\\frac{\\partial E}{\\partial a_1} &=& \\displaystyle\\frac{\\partial E}{\\partial b_1} \\displaystyle\\frac{\\partial b_1}{\\partial a_1} + \\displaystyle\\frac{\\partial E}{\\partial b_2} \\displaystyle\\frac{\\partial b_2}{\\partial a_1} \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "&=& \\displaystyle\\frac{\\partial E}{\\partial y_1} \\displaystyle\\frac{\\partial y_1}{\\partial b_1} \\displaystyle\\frac{\\partial b_1}{\\partial a_1} + \\displaystyle\\frac{\\partial E}{\\partial y_2} \\displaystyle\\frac{\\partial y_2}{\\partial b_2} \\displaystyle\\frac{\\partial b_2}{\\partial a_1} \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "&=& \\delta_1 \\, g(b_1)\\, \\displaystyle\\frac{\\partial b_1}{\\partial a_1} + \\delta_2 \\, g(b_2)\\, \\displaystyle\\frac{\\partial b_2}{\\partial a_1} \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\displaystyle\\frac{\\partial b_1}{\\partial a_1} &=& \\displaystyle\\frac{\\partial b_1}{\\partial z_1} \\displaystyle\\frac{\\partial z_1}{\\partial a_1} + \\displaystyle\\frac{\\partial b_1}{\\partial z_2} \\displaystyle\\frac{\\partial z_2}{\\partial a_1} = w_{11}^{(2)} \\, g(a_1) + w_{12}^{(2)} \\cdot 0 = w_{11}^{(2)} \\, g(a_1), \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial b_2}{\\partial a_1} &=& \\displaystyle\\frac{\\partial b_2}{\\partial z_1} \\displaystyle\\frac{\\partial z_1}{\\partial a_1} + \\displaystyle\\frac{\\partial b_2}{\\partial z_2} \\displaystyle\\frac{\\partial z_2}{\\partial a_1} = w_{21}^{(2)} \\, g(a_1) + w_{22}^{(2)} \\cdot 0 = w_{21}^{(2)} \\, g(a_1). \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tco1NQ488oRr"
      },
      "source": [
        "Similarly we have\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{11}^{(1)}} &=& \\displaystyle\\frac{\\partial E}{\\partial a_1} \\displaystyle\\frac{\\partial a_1}{\\partial w_{11}^{(1)}} = \\left\\{ \\delta_1\\, g(b_1) \\, w_{11}^{(2)} + \\delta_2\\, g(b_2) \\, w_{21}^{(2)} \\right\\} g(a_1)\\, x_1, \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{12}^{(1)}} &=& \\displaystyle\\frac{\\partial E}{\\partial a_1} \\displaystyle\\frac{\\partial a_1}{\\partial w_{12}^{(1)}} = \\left\\{ \\delta_1\\, g(b_1) \\, w_{11}^{(2)} + \\delta_2\\, g(b_2) \\, w_{21}^{(2)} \\right\\} g(a_1)\\, x_2, \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial \\beta_1^{(1)}} &=& \\displaystyle\\frac{\\partial E}{\\partial a_1} \\displaystyle\\frac{\\partial a_1}{\\partial \\beta_1^{(1)}} = \\left\\{ \\delta_1\\, g(b_1) \\, w_{11}^{(2)} + \\delta_2\\, g(b_2) \\, w_{21}^{(2)} \\right\\} g(a_1), \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{21}^{(1)}} &=& \\displaystyle\\frac{\\partial E}{\\partial a_2} \\displaystyle\\frac{\\partial a_2}{\\partial w_{21}^{(1)}} = \\left\\{ \\delta_1\\, g(b_1) \\, w_{12}^{(2)} + \\delta_2\\, g(b_2) \\, w_{22}^{(2)} \\right\\} g(a_2)\\, x_1, \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{22}^{(1)}} &=& \\displaystyle\\frac{\\partial E}{\\partial a_2} \\displaystyle\\frac{\\partial a_2}{\\partial w_{22}^{(1)}} = \\left\\{ \\delta_1\\, g(b_1) \\, w_{12}^{(2)} + \\delta_2\\, g(b_2) \\, w_{22}^{(2)} \\right\\} g(a_2)\\, x_2, \\nonumber \\\\\n",
        "& & \\nonumber \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial \\beta_2^{(1)}} &=& \\displaystyle\\frac{\\partial E}{\\partial a_2} \\displaystyle\\frac{\\partial a_2}{\\partial \\beta_2^{(1)}} = \\left\\{ \\delta_1\\, g(b_1) \\, w_{12}^{(2)} + \\delta_2\\, g(b_2) \\, w_{22}^{(2)} \\right\\} g(a_2). \\nonumber\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "Therefore we have them in matrix form as\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{11}^{(1)}} & \\displaystyle\\frac{\\partial E}{\\partial w_{12}^{(1)}} \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial w_{21}^{(1)}} & \\displaystyle\\frac{\\partial E}{\\partial w_{22}^{(1)}}\n",
        "\\end{bmatrix} = \\left\\{\n",
        "\\begin{bmatrix}\n",
        "g(a_1) \\\\\n",
        "g(a_2)\n",
        "\\end{bmatrix} \\odot \\left(\n",
        "\\begin{bmatrix}\n",
        "w_{11}^{(2)} & w_{12}^{(2)} \\\\\n",
        "w_{21}^{(2)} & w_{22}^{(2)}\n",
        "\\end{bmatrix}^T \\left(\n",
        "\\begin{bmatrix}\n",
        "g(b_1) \\\\\n",
        "g(b_2)\n",
        "\\end{bmatrix} \\odot\n",
        "\\begin{bmatrix}\n",
        "\\delta_1 \\\\\n",
        "\\delta_2\n",
        "\\end{bmatrix}\n",
        "\\right)\n",
        "\\right) \\right\\}\n",
        "\\begin{bmatrix}\n",
        "x_1 & x_2\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\displaystyle\\frac{\\partial E}{\\partial \\beta_{1}^{(1)}} \\\\\n",
        "\\displaystyle\\frac{\\partial E}{\\partial \\beta_{2}^{(1)}}\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "g(a_1) \\\\\n",
        "g(a_2)\n",
        "\\end{bmatrix} \\odot \\left(\n",
        "\\begin{bmatrix}\n",
        "w_{11}^{(2)} & w_{12}^{(2)} \\\\\n",
        "w_{21}^{(2)} & w_{22}^{(2)}\n",
        "\\end{bmatrix}^T \\left(\n",
        "\\begin{bmatrix}\n",
        "g(b_1) \\\\\n",
        "g(b_2)\n",
        "\\end{bmatrix} \\odot\n",
        "\\begin{bmatrix}\n",
        "\\delta_1 \\\\\n",
        "\\delta_2\n",
        "\\end{bmatrix}\n",
        "\\right)\n",
        "\\right).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8sladrh1VOM"
      },
      "source": [
        "### Summary\n",
        "The block diagram below shows the 2-layer NN model:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1AwIPuB8oXdvAR5QpYrmGGVgzK-lqEgRg)\n",
        "\n",
        "Based on the calculations above, the error backpropagation can be depicted as below:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=10jfdO5Fpl7AL0eN68TuHxI_HJqzo5kIN)\n",
        "\n",
        "In general, we can implement the forward and backward process of the linear layer and the activation function as shown below.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1_pebZ2eQCXK4HGITByvdNcXoeytq0IDO)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC7htcaaGrin"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}