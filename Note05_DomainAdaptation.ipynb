{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note05_DomainAdaptation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODn6u2eaJIDpsmxAF0yccR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiroTakeda/Notes/blob/main/Note05_DomainAdaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEuJUN3obJWI"
      },
      "source": [
        "# Domain Adaptation\n",
        "\n",
        "+ Related topics: __Regression__, __Logistic Regression__, __Neural Network__, __Generative Adversarial Net__\n",
        "\n",
        "Domain Adaptation using neural network was presented by Ganin *et al.* in 2016 [[paper](https://www.jmlr.org/papers/volume17/15-239/15-239.pdf)]. It is another important method for a variety of different problems. This short note briefly explains the proposed method.\n",
        "\n",
        "The peformance of the parametric approach strongly depends on the training dataset. When addressing the real world problems, we usually begin with collecting training data. Since data labeling is time-consuming, one might think whether it's possible to use existing datasets or to create datasets using computer graphics. However, the trained model with an existing dataset may not work well on the unlabelled dataset of our interest because the input data modality is different. For example, labeled images are all grayscale while unlabeled images are color. The domain adaptation technique with NN proposed by Ganin *et al.* allows us to train a model for unlabeled data using labled data that can be in a different form. The block diagram below shows the basic domain adaptation network architecture, in which the souce data are labeled and the target data are unlabeled. \n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1NZjcc6iGaSLxLBs1zPpEVHAk7cojn5FG)\n",
        "\n",
        "The objective of domain adaptation may be stated as:\n",
        "\n",
        "> Given a labelled (source) dataset and an unlabelled (target) dataset that can be in a different form, find a label predictor that is valid for both datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HriHQ89Bx4kX"
      },
      "source": [
        "## Network Model\n",
        "\n",
        "The basic domain adaptation model shown above consists of 3 components: feature extractor, label predictor, and domain classifier. The feature extractor converts the input to a feature vector, the label predictor estimates the class label using the feature vector, and the domain classifier classifies the feature vector whether it comes from either a source (labeled) or a target (unlabeled) input. Without the domain classifier, the netowrk model is nothing but a regular multinomial logistic regression.\n",
        "\n",
        "\n",
        "The objective is to have a label predctor that is valid for both souce (labeled) and target (unlabelled) data, but it is impossible to train the label predictor for target data without labels. One thing we can do is to match the probability distibutions of the features of source and target data: $p(F(\\mathbf{z})) = p(F(\\mathbf{x}))$. If they match, the label predictor works with unlabelled target data as well. The idea of probability distribution matching reminds us of __Generative Adversarial Net__ (GAN). The domain classifier is in fact identical to the discrminator of GAN, and therefore the opeimization process is similart to GAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVhdsR2p5ReK"
      },
      "source": [
        "## Optimization\n",
        "\n",
        "The dommain adaptation network consists of 3 components and we need to optimize them with different criteria. Usually the network is gradually optimized with the stochastic gradient method part by part as shown below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0h_tQ0J5eVZ"
      },
      "source": [
        "### Step 1: Label Predictor\n",
        "![picture](https://drive.google.com/uc?id=1YhSMMO2vFgpCI04quv8pPuZlcDGFZM3A)\n",
        "\n",
        "First, we optimize the feature extractor $F$ and the label predictor $L$ using the source (labeled) data. The combined network $L(F(\\cdot))$ is a multinomial logistic regression function and we can obtain the combined network by maximizing the log-likelihood function:\n",
        "\n",
        "$$\n",
        "\\displaystyle\\max_{L, F} \\displaystyle\\sum_{n} \\ln p(\\,l_n\\, |\\, L(F(\\mathbf{z}_n))\\,).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p2AskqD5jhD"
      },
      "source": [
        "### Step 2: Domain Classifier with source data\n",
        "![picture](https://drive.google.com/uc?id=1Whmju4wH7F-HLgTZle4kHJB2JM-tqKF2)\n",
        "\n",
        "Next, we optimize the domain classifier $D$ with $F$ fixed using the source data. Because we use the source data, the labels $t_n$ are $1$ for all $n$. Again the domain classifier is equivalent to the discriminator of GAN, and it is a binomial logistic regression function. We optimize $D$ by maximizing the log-likelihood function:\n",
        "\n",
        "$$\n",
        "\\displaystyle\\max_{D} \\displaystyle\\sum_{n} t_n \\ln p(\\,t_n \\, |\\, D(F(\\mathbf{z}_n)) \\,) \\color{lightgray}{+ \\displaystyle\\sum_{n} (1-t_n) \\ln p(\\,t_n\\, | \\,D(F(\\mathbf{x}_n)) \\,) },\n",
        "$$\n",
        "\n",
        "with $t_n=1$ (source)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_CvYDHC5nTc"
      },
      "source": [
        "### Step 3: Domain Classifier with target data\n",
        "![picture](https://drive.google.com/uc?id=1u8Hzmn5Fa4u7zSsXIpyQhXY4J3x1TjS2)\n",
        "\n",
        "Also we optimize the domain classifier $D$ with $F$ fixed using the target data. Because we use the target data, the labels $t_n$ are 0 for all n this time. We can use the same log-likelihood function for the optimization:\n",
        "\n",
        "$$\n",
        "\\displaystyle\\max_{D} \\color{lightgray}{\\displaystyle\\sum_{n} t_n \\ln p(\\,t_n\\, |\\, D(F(\\mathbf{z}_n)) \\,)} \\color{black}{+ \\displaystyle\\sum_{n} (1-t_n) \\ln p(\\,t_n\\, |\\, D(F(\\mathbf{x}_n)) \\,),}\n",
        "$$\n",
        "\n",
        "with $t_n=0$ (target).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu2geXeHTLks"
      },
      "source": [
        "### Step 4: Feature extractor with target data\n",
        "![picture](https://drive.google.com/uc?id=1xs9On1pinlEe0Mfb2nWvM3AEC-8ZWN2p)\n",
        "\n",
        "Finally, we optimize $F$ with $D$ fixed. We'd like the domain classifier $D$ to unwillingly classify the target feature $F(\\mathbf{x})$ as source so that the probability distributions of $F(\\mathbf{x})$ and $F(\\mathbf{z})$ match. As a result, the label predictor $L$ is valid for target data as well. To do that, we simply flip the label value $t_n$ to 1 (source), and then maximize the follwing log-likelihood function:\n",
        "\n",
        "$$\n",
        "\\displaystyle\\max_{F} \\displaystyle\\sum_n t_n \\ln p(\\, t_n \\, | \\, D(F(\\mathbf{x}_n)) \\, ),\n",
        "$$\n",
        "\n",
        "with $t_n=1$ (we want $F(\\mathbf{x})$ to be classifed as source).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhoLsFH4VqPH"
      },
      "source": [
        "## Inference\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1TU89EsaNG-UfLLoeR67VXlLl49--VAMi)\n",
        "\n",
        "Once the training is done, we no longer need the domain classifier, and the combined model of the feature extractor and the label predictor classifies the new input $\\mathbf{x}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVuad10whks9"
      },
      "source": [
        "## Example\n",
        "\n",
        "Below shows an example of the domain adaptation. This example uses \n",
        "MNIST dataset [[link](http://yann.lecun.com/exdb/mnist/)] as source (labeled) and MNIST-M datasets [[link](https://drive.google.com/drive/folders/0B_tExHiYS-0vR2dNZEU4NGlSSW8?resourcekey=0-Rs-0pTFZmKp_I1HoBkbiug)] as target (unlabeled) data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JrfX83_a9cS"
      },
      "source": [
        "import os\n",
        "import numpy\n",
        "import pylab\n",
        "import struct\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# MNIST dataset\n",
        "class MNISTdataset(Dataset):\n",
        "  def __init__(self, path_root, mode='train', transforms=None):\n",
        "    \n",
        "    self.transforms = transforms\n",
        "    \n",
        "    if mode == 'train':\n",
        "      path_images = '%s\\\\train-images.idx3-ubyte' % (path_root)\n",
        "      path_labels = '%s\\\\train-labels.idx1-ubyte' % (path_root)\n",
        "      num_images = 60000\n",
        "    else:\n",
        "      path_images = '%s\\\\t10k-images.idx3-ubyte' % (path_root)\n",
        "      path_labels = '%s\\\\t10k-labels.idx1-ubyte' % (path_root)\n",
        "      num_images = 10000\n",
        "      \n",
        "    with open(path_images, 'rb') as f:\n",
        "      self.images = numpy.zeros((28, 28, num_images), dtype=numpy.float32)\n",
        "      b = f.read(16)\n",
        "      for n in range(num_images):\n",
        "        b = f.read(28*28)\n",
        "        b = struct.unpack('>%dB' % (28*28), b)\n",
        "        self.images[:,:,n] = numpy.array(b).reshape(28, 28).astype(numpy.float32) / 255.0\n",
        "  \n",
        "    with open(path_labels, 'rb') as f:\n",
        "      b = f.read(8)\n",
        "      b = f.read(num_images)\n",
        "      b = struct.unpack('>%dB' % (num_images), b)\n",
        "      self.labels = numpy.array(b)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.labels.shape[0]\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img = self.images[:,:,index].reshape(28, 28, 1)\n",
        "    img = numpy.dstack((img, img, img)) # (28 x 28 x 1) --> (28 x 28 x 3)\n",
        "    img = numpy.pad(img, ((2,2), (2,2), (0,0)), 'symmetric') # (28 x 28 x 3) --> (32 x 32 x 3)\n",
        "    \n",
        "    lbl = self.labels[index].astype(numpy.int64)\n",
        "    \n",
        "    sample = {'image': img, 'label': lbl}\n",
        "    if self.transforms is not None:\n",
        "      sample = self.transforms(sample)\n",
        "      \n",
        "    return sample\n",
        "\n",
        "# MNIST-M dataset\n",
        "class MNISTMdataset(Dataset):\n",
        "  def __init__(self, path_root, mode='train', transforms=None):\n",
        "    \n",
        "    self.transforms = transforms\n",
        "    \n",
        "    # MNIST-M dataset\n",
        "    if mode == 'train':\n",
        "      path_images = '%s\\\\mnist_m_train' % (path_root)\n",
        "      path_labels = '%s\\\\mnist_m_train_labels.txt' % (path_root)\n",
        "    else:\n",
        "      path_images = '%s\\\\mnist_m_test' % (path_root)\n",
        "      path_labels = '%s\\\\mnist_m_test_labels.txt' % (path_root)\n",
        "      \n",
        "    with open(path_labels, 'rt') as f:\n",
        "      line = f.readline()\n",
        "      num_images = 0\n",
        "      while line:\n",
        "        parts = line.split(' ')\n",
        "        if os.path.exists('%s\\\\%s' % (path_images, parts[0])):\n",
        "          num_images += 1\n",
        "        line = f.readline()\n",
        "    \n",
        "    with open(path_labels, 'rt') as f:\n",
        "      self.images = numpy.zeros((32, 32, 3, num_images), dtype=numpy.float32)\n",
        "      self.labels = numpy.zeros(num_images, dtype=numpy.int)\n",
        "      line = f.readline()\n",
        "      n = 0\n",
        "      while line:\n",
        "        parts = line.split(' ')\n",
        "        if os.path.exists('%s\\\\%s' % (path_images, parts[0])):\n",
        "          self.images[:,:,:,n] = pylab.imread('%s\\\\%s' % (path_images, parts[0])).astype(numpy.float32)\n",
        "          if numpy.max(self.images[:,:,:,n]) > 1.0:\n",
        "            self.images[:,:,:,n] /= 255.0\n",
        "          self.labels[n] = int(parts[1])\n",
        "          n += 1\n",
        "        line = f.readline()\n",
        "    \n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.labels.shape[0]\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    \n",
        "    img = self.images[:,:,:,index].reshape(32, 32, 3)\n",
        "    \n",
        "    lbl = self.labels[index].astype(numpy.int64)\n",
        "    \n",
        "    sample = {'image': img, 'label': lbl}\n",
        "    if self.transforms is not None:\n",
        "      sample = self.transforms(sample)\n",
        "      \n",
        "    return sample\n",
        "  \n",
        "class myToTensor(object):\n",
        "  def __init__(self):\n",
        "    self.ToTensor = transforms.ToTensor()\n",
        "    \n",
        "  def __call__(self, sample):\n",
        "    sample['image'] = self.ToTensor(sample['image'])\n",
        "    return sample\n",
        "\n",
        "\n",
        "# Feature Extractor\n",
        "class FeatureExtractor(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, nf, ngpu):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.main = nn.Sequential(\n",
        "      # in_channels x 32 x 32\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 16 x 16\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 16 x 16\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 16 x 16\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 16 x 16\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 16 x 16\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 16 x 16\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 16 x 16\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf*2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf*2 x 8 x 8\n",
        "      nn.Conv2d(in_channels=nf*2, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf*2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf*2 x 8 x 8\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.main(x)\n",
        "\n",
        "\n",
        "# Label Predictor\n",
        "class LabelPredictor(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, nf, ngpu):\n",
        "    super(LabelPredictor, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.main = nn.Sequential(\n",
        "      # in_channels x 8 x 8\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 4 x 4\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 4 x 4\n",
        "      nn.Conv2d(in_channels=nf, out_channels=out_channels, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "      nn.LogSoftmax(dim=1)\n",
        "      # out_channels x 1 x 1\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.main(x)\n",
        "\n",
        "\n",
        "# Domain Classifier\n",
        "class DomainClassifier(nn.Module):\n",
        "  def __init__(self, in_channels, nf, ngpu):\n",
        "    super(DomainClassifier, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.main = nn.Sequential(\n",
        "      # in_channels x 8 x 8\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 4 x 4\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 4 x 4\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 4 x 4\n",
        "      nn.Conv2d(in_channels=nf, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(nf),\n",
        "      nn.ReLU(inplace=True),\n",
        "      # nf x 4 x 4\n",
        "      nn.Conv2d(in_channels=nf, out_channels=2, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "      nn.LogSoftmax(dim=1),\n",
        "      # 1 x 1 x 1\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.main(x)\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find('Conv') != -1:\n",
        "    nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "  elif classname.find('BatchNorm') != -1:\n",
        "    nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "    nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "def saveConfusionMatrix(fname, labels_true, labels_predicted):\n",
        "  pylab.ion()\n",
        "  cnf = confusion_matrix(labels_true, labels_predicted, normalize='true')\n",
        "  cnf = numpy.round(cnf*100, 0)\n",
        "  cmd = ConfusionMatrixDisplay(cnf)\n",
        "  cmd.plot()\n",
        "  pylab.show(block=False)\n",
        "  pylab.pause(1)\n",
        "  pylab.savefig(fname)\n",
        "  pylab.close()\n",
        "  return\n",
        "\n",
        "\n",
        "def main():\n",
        "  \n",
        "  path_mnist = 'F:\\\\datasets\\\\mnist'\n",
        "  path_mnistm = 'F:\\\\datasets\\\\mnist-m'\n",
        "  batch_size = 64\n",
        "  num_workers = 2\n",
        "  in_channels_FE = 3\n",
        "  out_channels_FE = 16\n",
        "  num_features_FE = 8\n",
        "  ngpu = 1\n",
        "  num_classes = 10\n",
        "  num_features_LP = num_features_FE * 2\n",
        "  in_channels_LP = out_channels_FE\n",
        "  in_channels_DC = in_channels_LP\n",
        "  num_features_DC = num_features_LP\n",
        "  learning_rate = 1e-4\n",
        "  num_epochs = 200\n",
        "  gamma = 0.99\n",
        "  \n",
        "  mnist_cnf_fig = 'F:\\\\MNIST_cnf.jpg'\n",
        "  mnistm_cnf_fig = 'F:\\\\MNIST-M_cnf.jpg'\n",
        "  loss_curve_fig = 'F:\\\\loss.jpg'\n",
        "  \n",
        "  if torch.cuda.is_available() and ngpu > 0:\n",
        "    device = torch.device('cuda:0')\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "  \n",
        "  mnist_train_transforms = transforms.Compose([\n",
        "      myToTensor(),\n",
        "    ])\n",
        "  \n",
        "  mnist_eval_transforms = transforms.Compose([\n",
        "      myToTensor(),\n",
        "    ])\n",
        "  \n",
        "  mnistm_train_transforms = transforms.Compose([\n",
        "      myToTensor(),\n",
        "    ])\n",
        "  \n",
        "  mnistm_eval_transforms = transforms.Compose([\n",
        "      myToTensor(),\n",
        "    ])\n",
        "  \n",
        "  mnist_train_dataset = MNISTdataset(path_mnist, mode='train', transforms=mnist_train_transforms)\n",
        "  mnist_eval_dataset = MNISTdataset(path_mnist, mode='eval', transforms=mnist_eval_transforms)\n",
        "  mnistm_train_dataset = MNISTMdataset(path_mnistm, mode='train', transforms=mnistm_train_transforms)\n",
        "  mnistm_eval_dataset = MNISTMdataset(path_mnistm, mode='eval', transforms=mnistm_eval_transforms)\n",
        "  \n",
        "  mnist_train_dataloader = DataLoader(dataset=mnist_train_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=True,\n",
        "                                      num_workers=num_workers)\n",
        "  \n",
        "  mnist_eval_dataloader = DataLoader(dataset=mnist_eval_dataset,\n",
        "                                     batch_size=batch_size,\n",
        "                                     shuffle=False,\n",
        "                                     num_workers=num_workers)\n",
        "  \n",
        "  mnistm_train_dataloader = DataLoader(dataset=mnistm_train_dataset,\n",
        "                                       batch_size=batch_size,\n",
        "                                       shuffle=True,\n",
        "                                       num_workers=num_workers)\n",
        "  \n",
        "  mnistm_eval_dataloader = DataLoader(dataset=mnistm_eval_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=False,\n",
        "                                      num_workers=num_workers)\n",
        "  \n",
        "  modelFE = FeatureExtractor(in_channels=in_channels_FE, out_channels=out_channels_FE, nf=num_features_FE, ngpu=ngpu)\n",
        "  modelLP = LabelPredictor(in_channels=in_channels_LP, out_channels=num_classes, nf=num_features_LP, ngpu=ngpu)\n",
        "  modelDC = DomainClassifier(in_channels=in_channels_DC, nf=num_features_DC, ngpu=ngpu)\n",
        "  \n",
        "  modelFE.apply(weights_init)\n",
        "  modelLP.apply(weights_init)\n",
        "  modelDC.apply(weights_init)\n",
        "  \n",
        "  modelFE.to(device)\n",
        "  modelLP.to(device)\n",
        "  modelDC.to(device)\n",
        "  \n",
        "  criterionLP = nn.NLLLoss()\n",
        "  criterionDC = nn.NLLLoss()\n",
        "  \n",
        "  optimizerFE = torch.optim.SGD(modelFE.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
        "  optimizerLP = torch.optim.SGD(modelLP.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
        "  optimizerDC = torch.optim.SGD(modelDC.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
        "  \n",
        "  schedulerFE = torch.optim.lr_scheduler.ExponentialLR(optimizerFE, gamma=gamma)\n",
        "  schedulerLP = torch.optim.lr_scheduler.ExponentialLR(optimizerLP, gamma=gamma)\n",
        "  schedulerDC = torch.optim.lr_scheduler.ExponentialLR(optimizerDC, gamma=gamma)\n",
        "  \n",
        "  source_label = 1\n",
        "  target_label = 0\n",
        "  \n",
        "  LP_train_losses = []\n",
        "  LP_eval_losses_1 = []\n",
        "  LP_eval_losses_2 = []\n",
        "  DC_train_losses_1 = []\n",
        "  DC_train_losses_2 = []\n",
        "  for epoch in range(num_epochs):\n",
        "    \n",
        "    # training\n",
        "    modelFE.train()\n",
        "    modelLP.train()\n",
        "    LP_loss = 0\n",
        "    DC_loss_1 = 0\n",
        "    DC_loss_2 = 0\n",
        "    mnist_train_iter = iter(mnist_train_dataloader)\n",
        "    for i, mnistm in enumerate(mnistm_train_dataloader, 0):\n",
        "      \n",
        "      # target batch\n",
        "      image_mnistm = mnistm['image'].to(device)\n",
        "      label_mnistm = mnistm['label'].to(device)\n",
        "      \n",
        "      # source batch\n",
        "      mnist = next(mnist_train_iter) #next(iter(mnist_train_dataloader))\n",
        "      image_mnist = mnist['image'].to(device)\n",
        "      label_mnist = mnist['label'].to(device)\n",
        "      \n",
        "      ######################################################\n",
        "      # Step 1: train the feature extractor and the label predictor with source data\n",
        "      modelFE.zero_grad()\n",
        "      modelLP.zero_grad()\n",
        "      f = modelFE(image_mnist)\n",
        "      y = modelLP(f)\n",
        "      y = y.reshape(y.shape[:2])\n",
        "      err = criterionLP(y, label_mnist)\n",
        "      err.backward()\n",
        "      optimizerFE.step()\n",
        "      optimizerLP.step()\n",
        "      \n",
        "      LP_loss += err.item() / len(label_mnist)\n",
        "      \n",
        "      if i % 100 == 0:\n",
        "        print(err.item())\n",
        "      \n",
        "      #######################################################\n",
        "      # Step 2: train the domain classifier with source data\n",
        "      modelFE.zero_grad()\n",
        "      modelDC.zero_grad()\n",
        "      b_size = label_mnist.shape[0]\n",
        "      label = torch.full((b_size,), source_label, dtype=torch.long, device=device)\n",
        "      f = modelFE(image_mnist)\n",
        "      y = modelDC(f)\n",
        "      y = y.reshape(y.shape[:2])\n",
        "      err1 = criterionDC(y, label)\n",
        "      \n",
        "      ########################################################\n",
        "      # Step 3: train the domain classifier with target data\n",
        "      b_size = label_mnistm.shape[0]\n",
        "      label = torch.full((b_size,), target_label, dtype=torch.long, device=device)\n",
        "      f = modelFE(image_mnistm)\n",
        "      y = modelDC(f)\n",
        "      y = y.reshape(y.shape[:2])\n",
        "      err2 = criterionDC(y, label)\n",
        "      err = err1 + err2\n",
        "      err.backward()\n",
        "      optimizerDC.step()\n",
        "      \n",
        "      DC_loss_1 += err.item() / len(label) / 2\n",
        "      \n",
        "      #######################################################\n",
        "      # Step 4: train the feature extractor with target data\n",
        "      # so that the domain classifier unwillingly classifies the feature vector was\n",
        "      # generated from the source data even though actually generated from the target data\n",
        "      modelFE.zero_grad()\n",
        "      modelDC.zero_grad()\n",
        "      b_size = label_mnistm.shape[0]\n",
        "      label = torch.full((b_size,), source_label, dtype=torch.long, device=device)\n",
        "      f = modelFE(image_mnistm)\n",
        "      y = modelDC(f)\n",
        "      y = y.reshape(y.shape[:2])\n",
        "      err = criterionDC(y, label)\n",
        "      err.backward()\n",
        "      optimizerFE.step()\n",
        "      \n",
        "      DC_loss_2 += err.item() / len(label)\n",
        "      \n",
        "    \n",
        "    LP_train_losses.append(LP_loss / len(mnistm_train_dataset))\n",
        "    DC_train_losses_1.append(DC_loss_1 / len(mnistm_train_dataset))\n",
        "    DC_train_losses_2.append(DC_loss_2 / len(mnistm_train_dataset))\n",
        "    \n",
        "    # update the learning rate\n",
        "    schedulerLP.step()\n",
        "    schedulerFE.step()\n",
        "    schedulerDC.step()\n",
        "    \n",
        "    \n",
        "    # evaluation\n",
        "    modelFE.eval()\n",
        "    modelLP.eval()\n",
        "    LP_loss = 0\n",
        "    labels_predicted = None\n",
        "    labels_true = None\n",
        "    for i, data in enumerate(mnist_eval_dataloader, 0):\n",
        "      \n",
        "      image = data['image'].to(device)\n",
        "      label = data['label'].to(device)\n",
        "      \n",
        "      f = modelFE(image)\n",
        "      y = modelLP(f)\n",
        "      y = y.reshape(y.shape[:2])\n",
        "      err = criterionLP(y, label)\n",
        "            \n",
        "      LP_loss += err.item() / len(label)\n",
        "      \n",
        "      pred = torch.argmax(y, dim=1).detach().cpu().numpy()\n",
        "      label = label.detach().cpu().numpy()\n",
        "      if labels_true is None:\n",
        "        labels_true = label.copy()\n",
        "        labels_predicted = pred.copy()\n",
        "      else:\n",
        "        labels_true = numpy.hstack((labels_true, label))\n",
        "        labels_predicted = numpy.hstack((labels_predicted, pred))\n",
        "      \n",
        "      if i % 100 == 0:\n",
        "        print(err.item())\n",
        "    \n",
        "    # plot the confusion matrix and save the figure    \n",
        "    saveConfusionMatrix(mnist_cnf_fig, labels_true, labels_predicted)\n",
        "    \n",
        "    LP_eval_losses_1.append(LP_loss / len(mnist_eval_dataset))\n",
        "    \n",
        "    LP_loss = 0\n",
        "    labels_predicted = None\n",
        "    labels_true = None\n",
        "    for i, data in enumerate(mnistm_eval_dataloader, 0):\n",
        "      \n",
        "      image = data['image'].to(device)\n",
        "      label = data['label'].to(device)\n",
        "      \n",
        "      f = modelFE(image)\n",
        "      y = modelLP(f)\n",
        "      y = y.reshape(y.shape[:2])\n",
        "      err = criterionLP(y, label)\n",
        "            \n",
        "      LP_loss += err.item() / len(label)\n",
        "      \n",
        "      pred = torch.argmax(y, dim=1).detach().cpu().numpy()\n",
        "      label = label.detach().cpu().numpy()\n",
        "      if labels_true is None:\n",
        "        labels_true = label.copy()\n",
        "        labels_predicted = pred.copy()\n",
        "      else:\n",
        "        labels_true = numpy.hstack((labels_true, label))\n",
        "        labels_predicted = numpy.hstack((labels_predicted, pred))\n",
        "      \n",
        "      if i % 100 == 0:\n",
        "        print(err.item())\n",
        "\n",
        "    # plot the confusion matrix and save the figure    \n",
        "    saveConfusionMatrix(mnistm_cnf_fig, labels_true, labels_predicted)\n",
        "\n",
        "    LP_eval_losses_2.append(LP_loss / len(mnistm_eval_dataset))\n",
        "    \n",
        "    pylab.ion()\n",
        "    fig = pylab.figure(0)\n",
        "    fig.clf()\n",
        "    pylab.plot(LP_train_losses, label='LP train loss')\n",
        "    pylab.plot(LP_eval_losses_1, label='LP eval loss 1')\n",
        "    pylab.plot(LP_eval_losses_2, label='LP eval loss 2')\n",
        "    pylab.plot(DC_train_losses_1, label='DC train loss 1')\n",
        "    pylab.plot(DC_train_losses_2, label='DC train loss 2')\n",
        "    pylab.legend()\n",
        "    pylab.grid()\n",
        "    pylab.show(block=False)\n",
        "    pylab.pause(1)\n",
        "    pylab.savefig(loss_curve_fig)\n",
        "  \n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVsBRdizoxHB"
      },
      "source": [
        "\n",
        "![picture](https://drive.google.com/uc?id=1BGehE1TChCp8gPvSexVZU-Y-BOrICcGl)\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=12xdvmS-HpgtCCcWHWZ-IK2IK4IW9Yv8Q)\n"
      ]
    }
  ]
}